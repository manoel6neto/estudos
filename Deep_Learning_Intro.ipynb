{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Intro",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manoel6neto/estudos/blob/master/Deep_Learning_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odi2vIMHC3Rm",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning: The Good, the Bad and the Ugly\n",
        "\n",
        "This notebook was created for PhD Open lectures given in June 2018 at the University of Warsaw. The slides for the lectures are available [here](http://phdopen.mimuw.edu.pl/lato18/w5s/DeepLearning.pdf) and the full recording [here on YouTube](https://www.youtube.com/channel/UCvMN-HLvvVa6lUXijCKFlqQ) - take a look at it if you need more explanation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir4JzdPlsD9M",
        "colab_type": "text"
      },
      "source": [
        "This notebook was created for PhD Open lectures given in June 2018 at the University of Warsaw. The slides for the lectures are available [here](http://phdopen.mimuw.edu.pl/lato18/w5s/DeepLearning.pdf) and the full recording [here on YouTube](https://www.youtube.com/channel/UCvMN-HLvvVa6lUXijCKFlqQ) - take a look at it if you need more explanation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPGni6fuvoTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install deps.\n",
        "!pip install -q -U tensor2tensor\n",
        "!pip install -q tensorflow matplotlib\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oILRLCWN_16u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports we need.\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import collections\n",
        "import random\n",
        "import six\n",
        "\n",
        "from tensor2tensor import models\n",
        "from tensor2tensor import problems\n",
        "from tensor2tensor.layers import common_attention\n",
        "from tensor2tensor.layers import common_layers\n",
        "from tensor2tensor.utils import trainer_lib\n",
        "from tensor2tensor.utils import t2t_model\n",
        "from tensor2tensor.utils import registry\n",
        "from tensor2tensor.utils import metrics\n",
        "from tensor2tensor.utils import learning_rate\n",
        "from tensor2tensor.utils import optimize\n",
        "\n",
        "# TF session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Other setup\n",
        "Modes = tf.estimator.ModeKeys\n",
        "\n",
        "# Setup some directories\n",
        "data_dir = os.path.expanduser(\"~/t2t/data\")\n",
        "tmp_dir = os.path.expanduser(\"~/t2t/tmp\")\n",
        "train_dir = os.path.expanduser(\"~/t2t/train\")\n",
        "checkpoint_dir = os.path.expanduser(\"~/t2t/checkpoints\")\n",
        "tf.gfile.MakeDirs(data_dir)\n",
        "tf.gfile.MakeDirs(tmp_dir)\n",
        "tf.gfile.MakeDirs(train_dir)\n",
        "tf.gfile.MakeDirs(checkpoint_dir)\n",
        "gs_data_dir = \"gs://tensor2tensor-data\"\n",
        "gs_ckpt_dir = \"gs://tensor2tensor-checkpoints/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a69r1KDiZDe",
        "colab_type": "text"
      },
      "source": [
        "# Download MNIST and inspect it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKc2uSk6WX5e",
        "colab_type": "code",
        "outputId": "ef5e8d10-bb85-44cb-934b-6532b6fcd32d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# Fetch the MNIST problem\n",
        "mnist_problem = problems.problem(\"image_mnist\")\n",
        "# The generate_data method of a problem will download data and process it into\n",
        "# a standard format ready for training and evaluation.\n",
        "mnist_problem.generate_data(data_dir, tmp_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Skipping generator because outputs files exist\n",
            "INFO:tensorflow:Skipping generator because outputs files exist\n",
            "INFO:tensorflow:Skipping shuffle because output files exist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW6HCRANFPYV",
        "colab_type": "code",
        "outputId": "9e11ee18-5dcb-4bec-be23-b05c45b0f8a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "# Now let's see the training MNIST data as Tensors.\n",
        "mnist_data = mnist_problem.dataset(Modes.TRAIN, data_dir)\n",
        "mnist_example_tensors = mnist_data.make_one_shot_iterator().get_next()\n",
        "mnist_example = sess.run(mnist_example_tensors)\n",
        "image = mnist_example[\"inputs\"]\n",
        "label = mnist_example[\"targets\"]\n",
        "\n",
        "plt.imshow(image[:, :, 0].astype(np.float32), cmap=plt.get_cmap('gray'))\n",
        "print(\"Label: %d\" % label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reading data files from /content/t2t/data/image_mnist-train*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 10\n",
            "Label: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngMH/AR3x+9ME0h6QAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f43a91788d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXL7_bVH49Kl",
        "colab_type": "text"
      },
      "source": [
        "# Basic MNIST model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQlCd98qijmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One-off all code together.\n",
        "\n",
        "# Data.\n",
        "BATCH_SIZE = 100\n",
        "data_batches = mnist_data.repeat().batch(BATCH_SIZE).make_one_shot_iterator()\n",
        "batch = data_batches.get_next()\n",
        "x, y = batch[\"inputs\"], batch[\"targets\"]\n",
        "x = tf.reshape(x, [BATCH_SIZE, 28*28])  # Height and width on channels.\n",
        "y = tf.squeeze(y, axis=1)  # Bogus dimension.\n",
        "# Model.\n",
        "hidden_size = 128\n",
        "h = tf.layers.dense(x, hidden_size, activation=tf.nn.relu, name=\"hidden\")\n",
        "o = tf.layers.dense(h, 10, activation=tf.nn.relu, name=\"output\")\n",
        "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=o, labels=y)\n",
        "accuracy = tf.to_float(tf.equal(tf.argmax(o, axis=-1), y))\n",
        "loss_t, accuracy_t = tf.reduce_mean(loss), tf.reduce_mean(accuracy)\n",
        "# Gradients.\n",
        "train_op = tf.train.AdamOptimizer().minimize(loss_t)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37NrUjoulXJz",
        "colab_type": "code",
        "outputId": "736de3f1-4be0-4d69-8251-e166b91bfe32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Train.\n",
        "num_steps = 1200  #  2 epochs on 60K examples.\n",
        "for step in range(num_steps):\n",
        "  _, loss, accuracy = sess.run([train_op, loss_t, accuracy_t])\n",
        "  if step % 100 == 0:\n",
        "    print(\"Step %d loss %.4f accuracy %.2f\" % (step, loss, accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 loss 2.5827 accuracy 0.09\n",
            "Step 100 loss 0.7940 accuracy 0.69\n",
            "Step 200 loss 0.4990 accuracy 0.84\n",
            "Step 300 loss 0.3487 accuracy 0.85\n",
            "Step 400 loss 0.3698 accuracy 0.86\n",
            "Step 500 loss 0.4084 accuracy 0.85\n",
            "Step 600 loss 0.4476 accuracy 0.85\n",
            "Step 700 loss 0.4722 accuracy 0.83\n",
            "Step 800 loss 0.2416 accuracy 0.92\n",
            "Step 900 loss 0.3838 accuracy 0.84\n",
            "Step 1000 loss 0.2252 accuracy 0.92\n",
            "Step 1100 loss 0.3039 accuracy 0.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xilLpmrK6wMB",
        "colab_type": "code",
        "outputId": "fb12da5f-0ddf-4c91-82e6-68689fc207ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "# Once again, all refactored and with reset.\n",
        "\n",
        "# Reset.\n",
        "sess.close()\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "\n",
        "# Data.\n",
        "problem = problems.problem(\"image_mnist\")\n",
        "problem.generate_data(data_dir, tmp_dir)\n",
        "BATCH_SIZE = 100\n",
        "train_data = problem.dataset(Modes.TRAIN, data_dir)\n",
        "train_batches = train_data.repeat().batch(BATCH_SIZE)\n",
        "train_batch = train_batches.make_one_shot_iterator().get_next()\n",
        "eval_data = problem.dataset(Modes.EVAL, data_dir)\n",
        "eval_batches = eval_data.repeat().batch(BATCH_SIZE)\n",
        "eval_batch = eval_batches.make_one_shot_iterator().get_next()\n",
        "\n",
        "# Model\n",
        "def model(batch, mode):\n",
        "  with tf.variable_scope(\"mymodel\", reuse=mode == Modes.EVAL):\n",
        "    # Inputs.\n",
        "    x, y = batch[\"inputs\"], batch[\"targets\"]\n",
        "    x = tf.reshape(x, [BATCH_SIZE, 28*28])  # Height and width on channels.\n",
        "    y = tf.squeeze(y, axis=1)  # Bogus dimension.\n",
        "    # Body.\n",
        "    hidden_size = 128\n",
        "    h = tf.layers.dense(x, hidden_size, activation=tf.nn.relu, name=\"hidden\")\n",
        "    o = tf.layers.dense(h, 10, activation=tf.nn.relu, name=\"output\")\n",
        "    # Loss and accuracy.\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=o, labels=y)\n",
        "    accuracy = tf.to_float(tf.equal(tf.argmax(o, axis=-1), y))\n",
        "    return tf.reduce_mean(loss), tf.reduce_mean(accuracy)\n",
        "\n",
        "# Model for train.\n",
        "train_loss, train_accuracy = model(train_batch, Modes.TRAIN)\n",
        "# Gradients.\n",
        "train_op = tf.train.AdamOptimizer().minimize(train_loss)\n",
        "# Model for eval.\n",
        "eval_loss, eval_accuracy = model(eval_batch, Modes.EVAL)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Skipping generator because outputs files exist\n",
            "INFO:tensorflow:Skipping generator because outputs files exist\n",
            "INFO:tensorflow:Skipping shuffle because output files exist\n",
            "INFO:tensorflow:Reading data files from /content/t2t/data/image_mnist-train*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 10\n",
            "INFO:tensorflow:Reading data files from /content/t2t/data/image_mnist-dev*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYiY9Qz09ZUl",
        "colab_type": "code",
        "outputId": "5be5102e-6e1a-47e0-82ec-47ce36a6e578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "# Train.\n",
        "sess.run(tf.global_variables_initializer())\n",
        "num_steps = 1200  #  2 epochs on 60K examples.\n",
        "for step in range(num_steps + 1):\n",
        "  _, loss, accuracy = sess.run([train_op, train_loss, train_accuracy])\n",
        "  if step % 100 == 0:\n",
        "    print(\"Step %d train loss %.4f accuracy %.2f\" % (step, loss, accuracy))\n",
        "    loss, accuracy = sess.run([eval_loss, eval_accuracy])\n",
        "    print(\"Step %d eval loss %.4f accuracy %.2f\" % (step, loss, accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 train loss 2.5899 accuracy 0.10\n",
            "Step 0 eval loss 2.3572 accuracy 0.14\n",
            "Step 100 train loss 0.2564 accuracy 0.94\n",
            "Step 100 eval loss 0.2812 accuracy 0.89\n",
            "Step 200 train loss 0.2011 accuracy 0.93\n",
            "Step 200 eval loss 0.1887 accuracy 0.93\n",
            "Step 300 train loss 0.2766 accuracy 0.91\n",
            "Step 300 eval loss 0.1942 accuracy 0.96\n",
            "Step 400 train loss 0.1238 accuracy 0.96\n",
            "Step 400 eval loss 0.1705 accuracy 0.95\n",
            "Step 500 train loss 0.1382 accuracy 0.95\n",
            "Step 500 eval loss 0.2202 accuracy 0.95\n",
            "Step 600 train loss 0.0463 accuracy 0.98\n",
            "Step 600 eval loss 0.1340 accuracy 0.96\n",
            "Step 700 train loss 0.0500 accuracy 0.98\n",
            "Step 700 eval loss 0.1345 accuracy 0.96\n",
            "Step 800 train loss 0.0859 accuracy 0.96\n",
            "Step 800 eval loss 0.1104 accuracy 0.97\n",
            "Step 900 train loss 0.0481 accuracy 0.99\n",
            "Step 900 eval loss 0.0404 accuracy 0.99\n",
            "Step 1000 train loss 0.0367 accuracy 1.00\n",
            "Step 1000 eval loss 0.1569 accuracy 0.93\n",
            "Step 1100 train loss 0.2330 accuracy 0.96\n",
            "Step 1100 eval loss 0.1891 accuracy 0.95\n",
            "Step 1200 train loss 0.0358 accuracy 0.99\n",
            "Step 1200 eval loss 0.1347 accuracy 0.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XWHdOtcRO7W",
        "colab_type": "text"
      },
      "source": [
        "# Deterministic sequence models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGbrR07mYFSh",
        "colab_type": "code",
        "outputId": "a733a214-59c0-4637-f121-04dd24dc3df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def generator(l):\n",
        "  inputs = list(np.random.randint(2, size=l))\n",
        "  even = [x for i, x in enumerate(inputs) if i % 2 == 0]\n",
        "  repeated = [[x, x] for x in even]\n",
        "  targets = [z for p in repeated for z in p]\n",
        "  yield {\"inputs\": inputs, \"targets\": targets}\n",
        "\n",
        "generator1 = lambda: generator(10)\n",
        "generator2 = lambda: generator(20)\n",
        "\n",
        "print(six.next(generator(6)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'inputs': [0, 0, 1, 0, 1, 1], 'targets': [0, 0, 1, 1, 1, 1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBhZ2-I1RXF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sequence model with 1 conv layer.\n",
        "\n",
        "# Reset.\n",
        "sess.close()\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "\n",
        "# Data.\n",
        "types = {\"inputs\": tf.int64, \"targets\": tf.int64}\n",
        "shapes = {\"inputs\": tf.TensorShape([None]), \"targets\": tf.TensorShape([None])}\n",
        "\n",
        "train_data = tf.data.Dataset.from_generator(\n",
        "    generator1, output_types=types, output_shapes=shapes)\n",
        "eval_data = tf.data.Dataset.from_generator(\n",
        "    generator2, output_types=types, output_shapes=shapes)\n",
        "BATCH_SIZE = 100\n",
        "train_batches = train_data.repeat().batch(BATCH_SIZE)\n",
        "train_batch = train_batches.make_one_shot_iterator().get_next()\n",
        "eval_batches = eval_data.repeat().batch(BATCH_SIZE)\n",
        "eval_batch = eval_batches.make_one_shot_iterator().get_next()\n",
        "\n",
        "# Model\n",
        "def model(batch, mode):\n",
        "  with tf.variable_scope(\"mymodel\", reuse=mode == Modes.EVAL):\n",
        "    # Inputs.\n",
        "    x, y = batch[\"inputs\"], batch[\"targets\"]\n",
        "    x = tf.reshape(x, [BATCH_SIZE, -1, 1])\n",
        "    y = tf.reshape(y, [BATCH_SIZE, -1, 1])\n",
        "    x_hot = tf.one_hot(x, 2)  # From ints to 1-hot vectors.\n",
        "    y_hot = tf.one_hot(y, 2)\n",
        "    x = tf.layers.dense(x_hot, 32, name=\"embedding_x\")\n",
        "    y_emb = tf.layers.dense(y_hot, 32, name=\"embedding_y\")\n",
        "    # Exercise: try enabling the 2 lines below.\n",
        "    # positions = tf.get_variable(\"positions\", [1, 20, 1, 32])\n",
        "    # x += positions[:, :tf.shape(x)[1], :, :]\n",
        "    # Body.\n",
        "    hidden_size = 32\n",
        "    h = tf.layers.conv2d(x, hidden_size, (3, 1),\n",
        "                         padding=\"same\", activation=tf.nn.relu, name=\"hidden\")\n",
        "    o = tf.layers.conv2d(h, 2, (1, 1),\n",
        "                         padding=\"same\", name=\"output\")\n",
        "    # Loss and accuracy.\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=o, labels=y)\n",
        "    accuracy = tf.to_float(tf.equal(tf.argmax(o, axis=-1), y))\n",
        "    return tf.reduce_mean(loss), tf.reduce_mean(accuracy)\n",
        "\n",
        "# Model for train.\n",
        "train_loss, train_accuracy = model(train_batch, Modes.TRAIN)\n",
        "# Gradients.\n",
        "train_op = tf.train.AdamOptimizer().minimize(train_loss)\n",
        "# Model for eval.\n",
        "eval_loss, eval_accuracy = model(eval_batch, Modes.EVAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXT8zEbpXaao",
        "colab_type": "code",
        "outputId": "1870681c-e885-4d31-c16d-dcb01cf683e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "# Train.\n",
        "sess.run(tf.global_variables_initializer())\n",
        "num_steps = 200\n",
        "for step in range(num_steps + 1):\n",
        "  _, loss, accuracy = sess.run([train_op, train_loss, train_accuracy])\n",
        "  if step % 10 == 0:\n",
        "    print(\"Step %d train loss %.4f accuracy %.2f\" % (step, loss, accuracy))\n",
        "    loss, accuracy = sess.run([eval_loss, eval_accuracy])\n",
        "    print(\"Step %d eval loss %.4f accuracy %.2f\" % (step, loss, accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 train loss 0.7191 accuracy 0.53\n",
            "Step 0 eval loss 0.7233 accuracy 0.49\n",
            "Step 10 train loss 0.5942 accuracy 0.76\n",
            "Step 10 eval loss 0.5742 accuracy 0.76\n",
            "Step 20 train loss 0.4912 accuracy 0.78\n",
            "Step 20 eval loss 0.4777 accuracy 0.76\n",
            "Step 30 train loss 0.4384 accuracy 0.75\n",
            "Step 30 eval loss 0.4101 accuracy 0.78\n",
            "Step 40 train loss 0.3727 accuracy 0.78\n",
            "Step 40 eval loss 0.3839 accuracy 0.75\n",
            "Step 50 train loss 0.3481 accuracy 0.77\n",
            "Step 50 eval loss 0.3406 accuracy 0.79\n",
            "Step 60 train loss 0.3269 accuracy 0.80\n",
            "Step 60 eval loss 0.3385 accuracy 0.77\n",
            "Step 70 train loss 0.3195 accuracy 0.81\n",
            "Step 70 eval loss 0.3308 accuracy 0.78\n",
            "Step 80 train loss 0.3115 accuracy 0.80\n",
            "Step 80 eval loss 0.3436 accuracy 0.76\n",
            "Step 90 train loss 0.3185 accuracy 0.80\n",
            "Step 90 eval loss 0.3315 accuracy 0.78\n",
            "Step 100 train loss 0.3176 accuracy 0.80\n",
            "Step 100 eval loss 0.3336 accuracy 0.77\n",
            "Step 110 train loss 0.3076 accuracy 0.80\n",
            "Step 110 eval loss 0.3350 accuracy 0.77\n",
            "Step 120 train loss 0.2904 accuracy 0.81\n",
            "Step 120 eval loss 0.3290 accuracy 0.78\n",
            "Step 130 train loss 0.3049 accuracy 0.80\n",
            "Step 130 eval loss 0.3105 accuracy 0.78\n",
            "Step 140 train loss 0.2945 accuracy 0.80\n",
            "Step 140 eval loss 0.3312 accuracy 0.76\n",
            "Step 150 train loss 0.2990 accuracy 0.79\n",
            "Step 150 eval loss 0.3199 accuracy 0.78\n",
            "Step 160 train loss 0.2996 accuracy 0.80\n",
            "Step 160 eval loss 0.3409 accuracy 0.76\n",
            "Step 170 train loss 0.2959 accuracy 0.80\n",
            "Step 170 eval loss 0.3148 accuracy 0.79\n",
            "Step 180 train loss 0.2942 accuracy 0.81\n",
            "Step 180 eval loss 0.3187 accuracy 0.78\n",
            "Step 190 train loss 0.2830 accuracy 0.82\n",
            "Step 190 eval loss 0.3228 accuracy 0.77\n",
            "Step 200 train loss 0.2845 accuracy 0.79\n",
            "Step 200 eval loss 0.3277 accuracy 0.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfLujWj4jZmc",
        "colab_type": "text"
      },
      "source": [
        "# Autoregressive sequence models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohk22OBUjiry",
        "colab_type": "code",
        "outputId": "b4559933-d9e3-4048-8034-4a405fd5a25e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def generator(l):\n",
        "  inputs = list(np.random.randint(2, size=l))\n",
        "  even = [x for i, x in enumerate(inputs) if i % 2 == 0]\n",
        "  repeated = [[x, x] for x in even]\n",
        "  targets1 = [z for p in repeated for z in p]\n",
        "  targets2 = inputs\n",
        "  targets = random.choice([targets1, targets2])\n",
        "  yield {\"inputs\": inputs, \"targets\": targets}\n",
        "\n",
        "generator1 = lambda: generator(10)\n",
        "generator2 = lambda: generator(20)\n",
        "\n",
        "print(six.next(generator(6)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'inputs': [0, 1, 1, 0, 0, 0], 'targets': [0, 1, 1, 0, 0, 0]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyBn6jzOjS0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sequence model with 1 conv layer.\n",
        "\n",
        "# Reset.\n",
        "sess.close()\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "\n",
        "# Data.\n",
        "types = {\"inputs\": tf.int64, \"targets\": tf.int64}\n",
        "shapes = {\"inputs\": tf.TensorShape([None]), \"targets\": tf.TensorShape([None])}\n",
        "\n",
        "train_data = tf.data.Dataset.from_generator(\n",
        "    generator1, output_types=types, output_shapes=shapes)\n",
        "eval_data = tf.data.Dataset.from_generator(\n",
        "    generator2, output_types=types, output_shapes=shapes)\n",
        "BATCH_SIZE = 100\n",
        "train_batches = train_data.repeat().batch(BATCH_SIZE)\n",
        "train_batch = train_batches.make_one_shot_iterator().get_next()\n",
        "eval_batches = eval_data.repeat().batch(BATCH_SIZE)\n",
        "eval_batch = eval_batches.make_one_shot_iterator().get_next()\n",
        "\n",
        "# Model\n",
        "def model(batch, mode):\n",
        "  with tf.variable_scope(\"mymodel\", reuse=mode == Modes.EVAL):\n",
        "    # Inputs.\n",
        "    x, y = batch[\"inputs\"], batch[\"targets\"]\n",
        "    x = tf.reshape(x, [BATCH_SIZE, -1, 1])\n",
        "    y = tf.reshape(y, [BATCH_SIZE, -1, 1])\n",
        "    x_hot = tf.one_hot(x, 2)  # From ints to 1-hot vectors.\n",
        "    y_hot = tf.one_hot(y, 2)\n",
        "    x = tf.layers.dense(x_hot, 32, name=\"embedding_x\")\n",
        "    y_emb = tf.layers.dense(y_hot, 32, name=\"embedding_y\")\n",
        "    positions = tf.scan(lambda a, z: tf.layers.dense(a, 32),\n",
        "                        tf.transpose(x, [1, 0, 2, 3]))\n",
        "    positions = tf.transpose(positions, [1, 0, 2, 3])\n",
        "    x += positions\n",
        "    # Body.\n",
        "    hidden_size = 32\n",
        "    h = tf.layers.conv2d(x, hidden_size, (3, 1),\n",
        "                         padding=\"same\", activation=tf.nn.relu, name=\"hidden\")\n",
        "    # Autoregressive part.\n",
        "    y_shifted = common_layers.shift_right(y_emb)\n",
        "    h += y_shifted\n",
        "    # Attention.\n",
        "    h = tf.expand_dims(tf.squeeze(h, axis=2), axis=1)\n",
        "    q = tf.layers.dense(h, 32, name=\"q\")\n",
        "    k = tf.layers.dense(h, 32, name=\"k\")\n",
        "    v = tf.layers.dense(h, 32, name=\"v\")\n",
        "    bias = common_attention.attention_bias_lower_triangle(\n",
        "              common_layers.shape_list(h)[1])\n",
        "    h += common_attention.dot_product_attention(q, k, v, bias)\n",
        "    h = tf.reshape(h, tf.shape(x))\n",
        "    # Logits.\n",
        "    o = tf.layers.conv2d(h, 2, (1, 1),\n",
        "                         padding=\"same\", name=\"output\")\n",
        "    # Loss and accuracy.\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=o, labels=y)\n",
        "    accuracy = tf.to_float(tf.equal(tf.argmax(o, axis=-1), y))\n",
        "    return tf.reduce_mean(loss), tf.reduce_mean(accuracy)\n",
        "\n",
        "# Model for train.\n",
        "train_loss, train_accuracy = model(train_batch, Modes.TRAIN)\n",
        "# Gradients.\n",
        "train_op = tf.train.AdamOptimizer().minimize(train_loss)\n",
        "# Model for eval.\n",
        "eval_loss, eval_accuracy = model(eval_batch, Modes.EVAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNhxmv4IZ4VU",
        "colab_type": "code",
        "outputId": "52287228-11e6-4616-cb28-a3c0d8be7f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        }
      },
      "source": [
        "# Train.\n",
        "sess.run(tf.global_variables_initializer())\n",
        "num_steps = 200\n",
        "for step in range(num_steps + 1):\n",
        "  _, loss, accuracy = sess.run([train_op, train_loss, train_accuracy])\n",
        "  if step % 10 == 0:\n",
        "    print(\"Step %d train loss %.4f accuracy %.2f\" % (step, loss, accuracy))\n",
        "    loss, accuracy = sess.run([eval_loss, eval_accuracy])\n",
        "    print(\"Step %d eval loss %.4f accuracy %.2f\" % (step, loss, accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 train loss 0.6352 accuracy 0.61\n",
            "Step 0 eval loss 0.6100 accuracy 0.62\n",
            "Step 10 train loss 0.4022 accuracy 0.85\n",
            "Step 10 eval loss 0.3863 accuracy 0.87\n",
            "Step 20 train loss 0.2453 accuracy 0.89\n",
            "Step 20 eval loss 0.2880 accuracy 0.85\n",
            "Step 30 train loss 0.1679 accuracy 0.92\n",
            "Step 30 eval loss 0.2939 accuracy 0.88\n",
            "Step 40 train loss 0.1139 accuracy 0.96\n",
            "Step 40 eval loss 0.1670 accuracy 0.92\n",
            "Step 50 train loss 0.0922 accuracy 0.96\n",
            "Step 50 eval loss 0.0954 accuracy 0.97\n",
            "Step 60 train loss 0.0632 accuracy 0.98\n",
            "Step 60 eval loss 0.1305 accuracy 0.95\n",
            "Step 70 train loss 0.0547 accuracy 0.98\n",
            "Step 70 eval loss 0.0590 accuracy 0.98\n",
            "Step 80 train loss 0.0596 accuracy 0.98\n",
            "Step 80 eval loss 0.0538 accuracy 0.98\n",
            "Step 90 train loss 0.0447 accuracy 0.98\n",
            "Step 90 eval loss 0.0332 accuracy 0.99\n",
            "Step 100 train loss 0.0424 accuracy 0.98\n",
            "Step 100 eval loss 0.0603 accuracy 0.98\n",
            "Step 110 train loss 0.0380 accuracy 0.98\n",
            "Step 110 eval loss 0.0333 accuracy 0.99\n",
            "Step 120 train loss 0.0263 accuracy 0.99\n",
            "Step 120 eval loss 0.0385 accuracy 0.99\n",
            "Step 130 train loss 0.0428 accuracy 0.98\n",
            "Step 130 eval loss 0.0473 accuracy 0.98\n",
            "Step 140 train loss 0.0352 accuracy 0.99\n",
            "Step 140 eval loss 0.0685 accuracy 0.98\n",
            "Step 150 train loss 0.0217 accuracy 0.99\n",
            "Step 150 eval loss 0.0712 accuracy 0.98\n",
            "Step 160 train loss 0.0287 accuracy 0.99\n",
            "Step 160 eval loss 0.1107 accuracy 0.97\n",
            "Step 170 train loss 0.0216 accuracy 1.00\n",
            "Step 170 eval loss 0.0995 accuracy 0.97\n",
            "Step 180 train loss 0.0306 accuracy 0.99\n",
            "Step 180 eval loss 0.1407 accuracy 0.96\n",
            "Step 190 train loss 0.0250 accuracy 0.99\n",
            "Step 190 eval loss 0.0648 accuracy 0.98\n",
            "Step 200 train loss 0.0342 accuracy 0.99\n",
            "Step 200 eval loss 0.0816 accuracy 0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNC9fVcnibcH",
        "colab_type": "text"
      },
      "source": [
        "# Run pre-trained translation Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB4MP7_y_SuQ",
        "colab_type": "code",
        "outputId": "39eef0c9-9542-404a-c6bf-8904cccf24d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Reset.\n",
        "sess.close()\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "\n",
        "# Fetch the problem\n",
        "# problems.available()\n",
        "ende_problem = problems.problem(\"translate_ende_wmt32k\")\n",
        "\n",
        "# Copy the vocab file locally so we can encode inputs and decode model outputs\n",
        "# All vocabs are stored on GCS\n",
        "vocab_name = \"vocab.ende.32768\"\n",
        "vocab_file = os.path.join(gs_data_dir, vocab_name)\n",
        "!gsutil cp {vocab_file} {data_dir}\n",
        "\n",
        "# Get the encoders from the problem\n",
        "encoders = ende_problem.feature_encoders(data_dir)\n",
        "\n",
        "# Setup helper functions for encoding and decoding\n",
        "def encode(input_str, output_str=None):\n",
        "  \"\"\"Input str to features dict, ready for inference\"\"\"\n",
        "  inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n",
        "  batch_inputs = tf.reshape(tf.constant(inputs), [1, -1, 1])  # Make it 3D.\n",
        "  return {\"inputs\": batch_inputs}\n",
        "\n",
        "def decode(integers):\n",
        "  \"\"\"List of ints to str\"\"\"\n",
        "  integers = list(np.squeeze(integers))\n",
        "  if 1 in integers:\n",
        "    integers = integers[:integers.index(1)]\n",
        "  return encoders[\"inputs\"].decode(np.squeeze(integers))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tensor2tensor-data/vocab.ende.32768...\n",
            "/ [1 files][316.4 KiB/316.4 KiB]                                                \n",
            "Operation completed over 1 objects/316.4 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2aQW7Z6TOEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Generate and view the data\n",
        "# # This cell is commented out because WMT data generation can take hours\n",
        "\n",
        "# ende_problem.generate_data(data_dir, tmp_dir)\n",
        "# example = tfe.Iterator(ende_problem.dataset(Modes.TRAIN, data_dir)).next()\n",
        "# inputs = [int(x) for x in example[\"inputs\"].numpy()] # Cast to ints.\n",
        "# targets = [int(x) for x in example[\"targets\"].numpy()] # Cast to ints.\n",
        "\n",
        "\n",
        "\n",
        "# # Example inputs as int-tensor.\n",
        "# print(\"Inputs, encoded:\")\n",
        "# print(inputs)\n",
        "# print(\"Inputs, decoded:\")\n",
        "# # Example inputs as a sentence.\n",
        "# print(decode(inputs))\n",
        "# # Example targets as int-tensor.\n",
        "# print(\"Targets, encoded:\")\n",
        "# print(targets)\n",
        "# # Example targets as a sentence.\n",
        "# print(\"Targets, decoded:\")\n",
        "# print(decode(targets))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l6hDQbrRUYV",
        "colab_type": "code",
        "outputId": "5a4728c2-5614-4a80-c0eb-a68acb94c4ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Create hparams and the model\n",
        "# registry.list_models()\n",
        "model_name = \"transformer\"\n",
        "hparams_set = \"transformer_base\"\n",
        "\n",
        "hparams = trainer_lib.create_hparams(hparams_set, data_dir=data_dir, problem_name=\"translate_ende_wmt32k\")\n",
        "\n",
        "# NOTE: Only create the model once when restoring from a checkpoint; it's a\n",
        "# Layer and so subsequent instantiations will have different variable scopes\n",
        "# that will not match the checkpoint.\n",
        "translate_model = registry.model(model_name)(hparams, Modes.EVAL)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
            "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEwNUVlMYOJi",
        "colab_type": "code",
        "outputId": "fc26cb5f-18e1-4dbb-8b51-d50d00559fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Copy the pretrained checkpoint locally\n",
        "ckpt_name = \"transformer_ende_test\"\n",
        "gs_ckpt = os.path.join(gs_ckpt_dir, ckpt_name)\n",
        "!gsutil -q cp -R {gs_ckpt} {checkpoint_dir}\n",
        "ckpt_path = tf.train.latest_checkpoint(os.path.join(checkpoint_dir, ckpt_name))\n",
        "ckpt_path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u'/content/t2t/checkpoints/transformer_ende_test/model.ckpt-3817425'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O-8E9d6TtuJ",
        "colab_type": "code",
        "outputId": "79046894-d8f1-4e14-e438-c7e66e869981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Restore and translate!\n",
        "def translate(inputs):\n",
        "  encoded_inputs = encode(inputs)\n",
        "  model_output = translate_model.infer(encoded_inputs)[\"outputs\"]\n",
        "  tf.train.Saver().restore(sess, ckpt_path)\n",
        "  return decode(sess.run(model_output))\n",
        "\n",
        "inputs = \"The animal didn't cross the street because it was too tired\"\n",
        "outputs = translate(inputs)\n",
        "\n",
        "print(\"Inputs: %s\" % inputs)\n",
        "print(\"Outputs: %s\" % outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Greedy Decoding\n",
            "INFO:tensorflow:Restoring parameters from /content/t2t/checkpoints/transformer_ende_test/model.ckpt-3817425\n",
            "Inputs: The animal didn't cross the street because it was too tired\n",
            "Outputs: Das Tier überquerte nicht die Straße, weil es zu müde war.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}